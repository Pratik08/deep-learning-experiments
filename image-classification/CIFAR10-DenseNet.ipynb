{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Classifying CIFAR-10 using the simple version of DenseNet. The model contains ~1 million parameters.\n",
    " - Code written with TensorFlow2.0-beta0\n",
    " - DenseNet Paper: https://arxiv.org/pdf/1608.06993.pdf\n",
    " - Original Implementation: https://github.com/liuzhuang13/DenseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(x, input_shape):\n",
    "  x = tf.keras.layers.BatchNormalization(input_shape=input_shape)(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(filters=growth_rate, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(x)\n",
    "  return x\n",
    "\n",
    "def dense_block(x, num_conv_layers):\n",
    "  concat_feat = x\n",
    "  for ctr in range(num_conv_layers):\n",
    "    x = conv_block(concat_feat, concat_feat.shape[1:])\n",
    "    concat_feat = tf.keras.layers.concatenate([concat_feat, x])\n",
    "  return concat_feat\n",
    "\n",
    "def transition_block(x):\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  x = tf.keras.layers.Conv2D(filters=x.shape[-1], kernel_size=1, strides=1, kernel_initializer='he_normal')(x)\n",
    "  x = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "  inputs = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "  \n",
    "  # Initial Conv1\n",
    "  x = tf.keras.layers.Conv2D(filters=16, kernel_size=3, strides=1, padding='same', kernel_initializer='he_normal')(inputs)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  \n",
    "  # Dense Block 1\n",
    "  x = dense_block(x, 12)\n",
    "  x = transition_block(x)\n",
    "  \n",
    "  # Dense Block 2\n",
    "  x = dense_block(x, 12)\n",
    "  x = transition_block(x)\n",
    "  \n",
    "  # Dense Block 3\n",
    "  x = dense_block(x, 12)\n",
    "  \n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Activation('relu')(x)\n",
    "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "  \n",
    "  x = tf.keras.layers.Dense(num_classes)(x)\n",
    "  outputs = tf.keras.layers.Activation('softmax')(x)\n",
    "  \n",
    "  model = tf.keras.Model(inputs, outputs)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "BATCH_SIZE = 256\n",
    "BUFFER = 10000\n",
    "growth_rate = 12\n",
    "lr_init = 0.1\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrate(epoch):\n",
    "  if (epoch > (EPOCHS / 2)):\n",
    "    return 0.01\n",
    "  elif (epoch > (0.75 * EPOCHS)):\n",
    "    return 0.001\n",
    "  return 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt = tf.keras.callbacks.ModelCheckpoint('.', monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq=5)\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=150, verbose=1, mode='auto', baseline=None, restore_best_weights=True)\n",
    "lrate_sched = tf.keras.callbacks.LearningRateScheduler(lrate, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# Input image dimensions.\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "x_train_mean = np.mean(x_train, axis=0)\n",
    "x_train -= x_train_mean\n",
    "x_test -= x_train_mean\n",
    "\n",
    "# Convert to One-Hot Encoding format.\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        # randomly shift images horizontally (fraction of total width)\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically (fraction of total height)\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.,  # set range for random shear\n",
    "        zoom_range=0.,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.0)\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   448         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 12)   1740        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 28)   0           activation[0][0]                 \n",
      "                                                                 conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 28)   112         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, 32, 28)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 12)   3036        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 40)   0           concatenate[0][0]                \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 40)   160         concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 32, 32, 40)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 12)   4332        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 32, 32, 52)   0           concatenate_1[0][0]              \n",
      "                                                                 conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 52)   208         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 32, 32, 52)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 12)   5628        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 32, 32, 64)   0           concatenate_2[0][0]              \n",
      "                                                                 conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 32, 32, 64)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 12)   6924        activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 76)   0           concatenate_3[0][0]              \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 76)   304         concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 76)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 12)   8220        activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 32, 32, 88)   0           concatenate_4[0][0]              \n",
      "                                                                 conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 88)   352         concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 32, 32, 88)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 12)   9516        activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 32, 32, 100)  0           concatenate_5[0][0]              \n",
      "                                                                 conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 100)  400         concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 32, 32, 100)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 12)   10812       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 32, 32, 112)  0           concatenate_6[0][0]              \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 112)  448         concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 32, 32, 112)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 12)   12108       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 32, 32, 124)  0           concatenate_7[0][0]              \n",
      "                                                                 conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 124)  496         concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 32, 32, 124)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 12)   13404       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 32, 32, 136)  0           concatenate_8[0][0]              \n",
      "                                                                 conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 136)  544         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 32, 32, 136)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 12)   14700       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 32, 32, 148)  0           concatenate_9[0][0]              \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 148)  592         concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, 32, 148)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 12)   15996       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 32, 32, 160)  0           concatenate_10[0][0]             \n",
      "                                                                 conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 160)  640         concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 32, 32, 160)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 160)  25760       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d (AveragePooli (None, 16, 16, 160)  0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 16, 16, 160)  640         average_pooling2d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 16, 16, 160)  0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 16, 16, 12)   17292       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 16, 16, 172)  0           average_pooling2d[0][0]          \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 16, 16, 172)  688         concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 16, 16, 172)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 16, 16, 12)   18588       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 16, 16, 184)  0           concatenate_12[0][0]             \n",
      "                                                                 conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 184)  736         concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 16, 16, 184)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 12)   19884       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_14 (Concatenate)    (None, 16, 16, 196)  0           concatenate_13[0][0]             \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 196)  784         concatenate_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 16, 16, 196)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 12)   21180       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_15 (Concatenate)    (None, 16, 16, 208)  0           concatenate_14[0][0]             \n",
      "                                                                 conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 208)  832         concatenate_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 16, 16, 208)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 12)   22476       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_16 (Concatenate)    (None, 16, 16, 220)  0           concatenate_15[0][0]             \n",
      "                                                                 conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 220)  880         concatenate_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 16, 16, 220)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 12)   23772       activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_17 (Concatenate)    (None, 16, 16, 232)  0           concatenate_16[0][0]             \n",
      "                                                                 conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 232)  928         concatenate_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 16, 16, 232)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 12)   25068       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_18 (Concatenate)    (None, 16, 16, 244)  0           concatenate_17[0][0]             \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 244)  976         concatenate_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 16, 16, 244)  0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 12)   26364       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_19 (Concatenate)    (None, 16, 16, 256)  0           concatenate_18[0][0]             \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 256)  1024        concatenate_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 16, 16, 256)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 12)   27660       activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 16, 16, 268)  0           concatenate_19[0][0]             \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 268)  1072        concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 16, 16, 268)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 12)   28956       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_21 (Concatenate)    (None, 16, 16, 280)  0           concatenate_20[0][0]             \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 280)  1120        concatenate_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 16, 16, 280)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 12)   30252       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_22 (Concatenate)    (None, 16, 16, 292)  0           concatenate_21[0][0]             \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 292)  1168        concatenate_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 16, 16, 292)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 12)   31548       activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_23 (Concatenate)    (None, 16, 16, 304)  0           concatenate_22[0][0]             \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 304)  1216        concatenate_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 16, 16, 304)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 304)  92720       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 8, 8, 304)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 8, 8, 304)    1216        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 304)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 8, 12)     32844       activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 8, 8, 316)    0           average_pooling2d_1[0][0]        \n",
      "                                                                 conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 8, 8, 316)    1264        concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 8, 8, 316)    0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 8, 12)     34140       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 328)    0           concatenate_24[0][0]             \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 8, 8, 328)    1312        concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 8, 8, 328)    0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 8, 8, 12)     35436       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 8, 8, 340)    0           concatenate_25[0][0]             \n",
      "                                                                 conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 8, 8, 340)    1360        concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 8, 8, 340)    0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 8, 8, 12)     36732       activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 8, 8, 352)    0           concatenate_26[0][0]             \n",
      "                                                                 conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 8, 8, 352)    1408        concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 8, 8, 352)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 8, 8, 12)     38028       activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_28 (Concatenate)    (None, 8, 8, 364)    0           concatenate_27[0][0]             \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 8, 8, 364)    1456        concatenate_28[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 8, 8, 364)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 8, 8, 12)     39324       activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 8, 8, 376)    0           concatenate_28[0][0]             \n",
      "                                                                 conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 8, 8, 376)    1504        concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 8, 8, 376)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 8, 8, 12)     40620       activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_30 (Concatenate)    (None, 8, 8, 388)    0           concatenate_29[0][0]             \n",
      "                                                                 conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 8, 8, 388)    1552        concatenate_30[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 8, 8, 388)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 8, 8, 12)     41916       activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_31 (Concatenate)    (None, 8, 8, 400)    0           concatenate_30[0][0]             \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 8, 8, 400)    1600        concatenate_31[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 8, 8, 400)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 8, 8, 12)     43212       activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_32 (Concatenate)    (None, 8, 8, 412)    0           concatenate_31[0][0]             \n",
      "                                                                 conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 8, 8, 412)    1648        concatenate_32[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 8, 8, 412)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 8, 12)     44508       activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 8, 8, 424)    0           concatenate_32[0][0]             \n",
      "                                                                 conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 8, 8, 424)    1696        concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 8, 8, 424)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 8, 12)     45804       activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 436)    0           concatenate_33[0][0]             \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 8, 8, 436)    1744        concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 8, 8, 436)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 8, 8, 12)     47100       activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 8, 8, 448)    0           concatenate_34[0][0]             \n",
      "                                                                 conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 8, 8, 448)    1792        concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 8, 8, 448)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 448)          0           activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           4490        global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 10)           0           dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 1,038,794\n",
      "Trainable params: 1,020,666\n",
      "Non-trainable params: 18,128\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = tf.keras.optimizers.SGD(lr=lr_init, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 1/300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0618 05:30:50.892235 139767861499712 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/195 [==============================] - 126s 645ms/step - loss: 1.3750 - accuracy: 0.4974 - val_loss: 3.7497 - val_accuracy: 0.3277\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 2/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.9329 - accuracy: 0.6686 - val_loss: 1.3734 - val_accuracy: 0.5858\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 3/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.7544 - accuracy: 0.7354 - val_loss: 0.9816 - val_accuracy: 0.6719\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 4/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.6489 - accuracy: 0.7744 - val_loss: 0.9042 - val_accuracy: 0.6988\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 5/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.5753 - accuracy: 0.7986 - val_loss: 0.8113 - val_accuracy: 0.7329\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 6/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.5259 - accuracy: 0.8182 - val_loss: 0.6223 - val_accuracy: 0.7899\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 7/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.4808 - accuracy: 0.8330 - val_loss: 1.2450 - val_accuracy: 0.6608\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 8/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.4455 - accuracy: 0.8464 - val_loss: 0.9715 - val_accuracy: 0.7216\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 9/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.4178 - accuracy: 0.8560 - val_loss: 0.8375 - val_accuracy: 0.7511\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 10/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.3935 - accuracy: 0.8639 - val_loss: 0.7011 - val_accuracy: 0.7846\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 11/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.3701 - accuracy: 0.8714 - val_loss: 0.7958 - val_accuracy: 0.7621\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 12/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.3497 - accuracy: 0.8788 - val_loss: 0.5776 - val_accuracy: 0.8190\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 13/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.3378 - accuracy: 0.8826 - val_loss: 0.6174 - val_accuracy: 0.8006\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 14/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.3169 - accuracy: 0.8896 - val_loss: 0.6023 - val_accuracy: 0.8161\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 15/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.3043 - accuracy: 0.8950 - val_loss: 0.7186 - val_accuracy: 0.7961\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 16/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.2888 - accuracy: 0.8985 - val_loss: 0.5562 - val_accuracy: 0.8212\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 17/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.2827 - accuracy: 0.9017 - val_loss: 0.6541 - val_accuracy: 0.8176\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 18/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.2681 - accuracy: 0.9073 - val_loss: 0.5151 - val_accuracy: 0.8318\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 19/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.2570 - accuracy: 0.9110 - val_loss: 0.5087 - val_accuracy: 0.8362\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 20/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.2466 - accuracy: 0.9135 - val_loss: 0.6639 - val_accuracy: 0.8099\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 21/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.2381 - accuracy: 0.9184 - val_loss: 0.4603 - val_accuracy: 0.8649\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 22/300\n",
      "196/195 [==============================] - 101s 514ms/step - loss: 0.2317 - accuracy: 0.9185 - val_loss: 0.6512 - val_accuracy: 0.8123\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 23/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.2184 - accuracy: 0.9227 - val_loss: 0.4932 - val_accuracy: 0.8481\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 24/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.2105 - accuracy: 0.9265 - val_loss: 0.5353 - val_accuracy: 0.8424\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 25/300\n",
      "196/195 [==============================] - 100s 512ms/step - loss: 0.2067 - accuracy: 0.9277 - val_loss: 0.5315 - val_accuracy: 0.8362\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 26/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.1994 - accuracy: 0.9297 - val_loss: 0.6261 - val_accuracy: 0.8354\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 27/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1924 - accuracy: 0.9314 - val_loss: 0.5163 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 28/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1826 - accuracy: 0.9352 - val_loss: 0.5183 - val_accuracy: 0.8544\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 29/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1839 - accuracy: 0.9349 - val_loss: 0.6046 - val_accuracy: 0.8320\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 30/300\n",
      "196/195 [==============================] - 100s 512ms/step - loss: 0.1779 - accuracy: 0.9363 - val_loss: 0.7260 - val_accuracy: 0.8124\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 31/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1702 - accuracy: 0.9401 - val_loss: 1.0178 - val_accuracy: 0.7675\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 32/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1619 - accuracy: 0.9429 - val_loss: 0.4684 - val_accuracy: 0.8699\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 33/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1615 - accuracy: 0.9438 - val_loss: 0.6142 - val_accuracy: 0.8366\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 34/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.1563 - accuracy: 0.9446 - val_loss: 0.6173 - val_accuracy: 0.8380\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 35/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.1520 - accuracy: 0.9454 - val_loss: 0.4934 - val_accuracy: 0.8575\n",
      "\n",
      "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 36/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.1500 - accuracy: 0.9467 - val_loss: 0.5273 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 37/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.1403 - accuracy: 0.9509 - val_loss: 0.6553 - val_accuracy: 0.8192\n",
      "\n",
      "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 38/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.1439 - accuracy: 0.9482 - val_loss: 0.5977 - val_accuracy: 0.8421\n",
      "\n",
      "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 39/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.1327 - accuracy: 0.9529 - val_loss: 0.5401 - val_accuracy: 0.8570\n",
      "\n",
      "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 40/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.1339 - accuracy: 0.9520 - val_loss: 0.6351 - val_accuracy: 0.8495\n",
      "\n",
      "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 41/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1315 - accuracy: 0.9530 - val_loss: 0.5264 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 42/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.1276 - accuracy: 0.9543 - val_loss: 0.5394 - val_accuracy: 0.8534\n",
      "\n",
      "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 43/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.1270 - accuracy: 0.9548 - val_loss: 0.5020 - val_accuracy: 0.8687\n",
      "\n",
      "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 44/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.1184 - accuracy: 0.9579 - val_loss: 0.5352 - val_accuracy: 0.8600\n",
      "\n",
      "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 45/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.1160 - accuracy: 0.9588 - val_loss: 0.4970 - val_accuracy: 0.8666\n",
      "\n",
      "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 46/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1070 - accuracy: 0.9616 - val_loss: 0.5745 - val_accuracy: 0.8551\n",
      "\n",
      "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 47/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.1133 - accuracy: 0.9590 - val_loss: 0.4875 - val_accuracy: 0.8669\n",
      "\n",
      "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 48/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1054 - accuracy: 0.9619 - val_loss: 0.4716 - val_accuracy: 0.8785\n",
      "\n",
      "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 49/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.1051 - accuracy: 0.9620 - val_loss: 0.3678 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 00050: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 50/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1034 - accuracy: 0.9634 - val_loss: 0.4597 - val_accuracy: 0.8757\n",
      "\n",
      "Epoch 00051: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 51/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.1023 - accuracy: 0.9629 - val_loss: 0.4771 - val_accuracy: 0.8756\n",
      "\n",
      "Epoch 00052: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 52/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0983 - accuracy: 0.9653 - val_loss: 0.4525 - val_accuracy: 0.8774\n",
      "\n",
      "Epoch 00053: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 53/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0955 - accuracy: 0.9664 - val_loss: 0.5356 - val_accuracy: 0.8684\n",
      "\n",
      "Epoch 00054: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 54/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0931 - accuracy: 0.9674 - val_loss: 0.6245 - val_accuracy: 0.8513\n",
      "\n",
      "Epoch 00055: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 55/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0893 - accuracy: 0.9677 - val_loss: 0.5855 - val_accuracy: 0.8630\n",
      "\n",
      "Epoch 00056: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 56/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0957 - accuracy: 0.9659 - val_loss: 0.6126 - val_accuracy: 0.8596\n",
      "\n",
      "Epoch 00057: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 57/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0838 - accuracy: 0.9700 - val_loss: 0.4826 - val_accuracy: 0.8767\n",
      "\n",
      "Epoch 00058: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 58/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0854 - accuracy: 0.9699 - val_loss: 0.7295 - val_accuracy: 0.8514\n",
      "\n",
      "Epoch 00059: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 59/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0825 - accuracy: 0.9707 - val_loss: 0.3964 - val_accuracy: 0.8957\n",
      "\n",
      "Epoch 00060: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 60/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0853 - accuracy: 0.9696 - val_loss: 0.8895 - val_accuracy: 0.8291\n",
      "\n",
      "Epoch 00061: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 61/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0836 - accuracy: 0.9706 - val_loss: 0.5199 - val_accuracy: 0.8739\n",
      "\n",
      "Epoch 00062: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 62/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0788 - accuracy: 0.9720 - val_loss: 0.5189 - val_accuracy: 0.8740\n",
      "\n",
      "Epoch 00063: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 63/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0766 - accuracy: 0.9730 - val_loss: 0.6024 - val_accuracy: 0.8573\n",
      "\n",
      "Epoch 00064: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 64/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0760 - accuracy: 0.9734 - val_loss: 0.4264 - val_accuracy: 0.8936\n",
      "\n",
      "Epoch 00065: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 65/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0741 - accuracy: 0.9741 - val_loss: 0.4363 - val_accuracy: 0.8866\n",
      "\n",
      "Epoch 00066: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 66/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0705 - accuracy: 0.9749 - val_loss: 0.5976 - val_accuracy: 0.8692\n",
      "\n",
      "Epoch 00067: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 67/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0694 - accuracy: 0.9752 - val_loss: 0.4464 - val_accuracy: 0.8912\n",
      "\n",
      "Epoch 00068: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 68/300\n",
      "196/195 [==============================] - 100s 512ms/step - loss: 0.0733 - accuracy: 0.9749 - val_loss: 0.5605 - val_accuracy: 0.8689\n",
      "\n",
      "Epoch 00069: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 69/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0653 - accuracy: 0.9765 - val_loss: 0.5738 - val_accuracy: 0.8733\n",
      "\n",
      "Epoch 00070: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 70/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0675 - accuracy: 0.9761 - val_loss: 0.4557 - val_accuracy: 0.8889\n",
      "\n",
      "Epoch 00071: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 71/300\n",
      "196/195 [==============================] - 100s 512ms/step - loss: 0.0680 - accuracy: 0.9761 - val_loss: 0.4464 - val_accuracy: 0.8914\n",
      "\n",
      "Epoch 00072: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 72/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0620 - accuracy: 0.9778 - val_loss: 0.4879 - val_accuracy: 0.8848\n",
      "\n",
      "Epoch 00073: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 73/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0672 - accuracy: 0.9765 - val_loss: 0.5021 - val_accuracy: 0.8865\n",
      "\n",
      "Epoch 00074: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 74/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0621 - accuracy: 0.9777 - val_loss: 0.6012 - val_accuracy: 0.8784\n",
      "\n",
      "Epoch 00075: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 75/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0612 - accuracy: 0.9784 - val_loss: 0.3968 - val_accuracy: 0.9022\n",
      "\n",
      "Epoch 00076: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 76/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0613 - accuracy: 0.9782 - val_loss: 0.4637 - val_accuracy: 0.8861\n",
      "\n",
      "Epoch 00077: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 77/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0608 - accuracy: 0.9785 - val_loss: 0.5067 - val_accuracy: 0.8853\n",
      "\n",
      "Epoch 00078: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 78/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0617 - accuracy: 0.9778 - val_loss: 0.6870 - val_accuracy: 0.8588\n",
      "\n",
      "Epoch 00079: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 79/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0610 - accuracy: 0.9784 - val_loss: 0.5411 - val_accuracy: 0.8812\n",
      "\n",
      "Epoch 00080: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 80/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 0.4172 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00081: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 81/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0557 - accuracy: 0.9806 - val_loss: 0.4645 - val_accuracy: 0.8887\n",
      "\n",
      "Epoch 00082: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 82/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0546 - accuracy: 0.9809 - val_loss: 0.5067 - val_accuracy: 0.8946\n",
      "\n",
      "Epoch 00083: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 83/300\n",
      "196/195 [==============================] - 99s 505ms/step - loss: 0.0562 - accuracy: 0.9806 - val_loss: 0.4543 - val_accuracy: 0.9011\n",
      "\n",
      "Epoch 00084: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 84/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0513 - accuracy: 0.9824 - val_loss: 0.5722 - val_accuracy: 0.8803\n",
      "\n",
      "Epoch 00085: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 85/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 0.6034 - val_accuracy: 0.8785\n",
      "\n",
      "Epoch 00086: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 86/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0516 - accuracy: 0.9811 - val_loss: 0.5477 - val_accuracy: 0.8787\n",
      "\n",
      "Epoch 00087: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 87/300\n",
      "196/195 [==============================] - 99s 505ms/step - loss: 0.0518 - accuracy: 0.9821 - val_loss: 0.5455 - val_accuracy: 0.8865\n",
      "\n",
      "Epoch 00088: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 88/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0505 - accuracy: 0.9824 - val_loss: 0.6068 - val_accuracy: 0.8743\n",
      "\n",
      "Epoch 00089: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 89/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0488 - accuracy: 0.9825 - val_loss: 0.4773 - val_accuracy: 0.8950\n",
      "\n",
      "Epoch 00090: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 90/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0520 - accuracy: 0.9811 - val_loss: 0.6874 - val_accuracy: 0.8555\n",
      "\n",
      "Epoch 00091: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 91/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0468 - accuracy: 0.9830 - val_loss: 0.5186 - val_accuracy: 0.8848\n",
      "\n",
      "Epoch 00092: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 92/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0461 - accuracy: 0.9835 - val_loss: 0.6564 - val_accuracy: 0.8658\n",
      "\n",
      "Epoch 00093: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 93/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0493 - accuracy: 0.9831 - val_loss: 0.4937 - val_accuracy: 0.8963\n",
      "\n",
      "Epoch 00094: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 94/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0488 - accuracy: 0.9824 - val_loss: 0.6425 - val_accuracy: 0.8693\n",
      "\n",
      "Epoch 00095: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 95/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0468 - accuracy: 0.9834 - val_loss: 0.5768 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00096: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 96/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0404 - accuracy: 0.9858 - val_loss: 0.4534 - val_accuracy: 0.8969\n",
      "\n",
      "Epoch 00097: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 97/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0383 - accuracy: 0.9866 - val_loss: 0.5354 - val_accuracy: 0.8837\n",
      "\n",
      "Epoch 00098: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 98/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0444 - accuracy: 0.9846 - val_loss: 0.6994 - val_accuracy: 0.8697\n",
      "\n",
      "Epoch 00099: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 99/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0456 - accuracy: 0.9840 - val_loss: 0.4864 - val_accuracy: 0.8933\n",
      "\n",
      "Epoch 00100: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 100/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0423 - accuracy: 0.9851 - val_loss: 0.5478 - val_accuracy: 0.8869\n",
      "\n",
      "Epoch 00101: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 101/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0417 - accuracy: 0.9858 - val_loss: 0.4083 - val_accuracy: 0.9010\n",
      "\n",
      "Epoch 00102: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 102/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0390 - accuracy: 0.9860 - val_loss: 0.5464 - val_accuracy: 0.8832\n",
      "\n",
      "Epoch 00103: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 103/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0404 - accuracy: 0.9854 - val_loss: 0.5238 - val_accuracy: 0.8963\n",
      "\n",
      "Epoch 00104: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 104/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0388 - accuracy: 0.9864 - val_loss: 0.6528 - val_accuracy: 0.8752\n",
      "\n",
      "Epoch 00105: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 105/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0378 - accuracy: 0.9865 - val_loss: 0.5224 - val_accuracy: 0.8949\n",
      "\n",
      "Epoch 00106: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 106/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0437 - accuracy: 0.9850 - val_loss: 0.6049 - val_accuracy: 0.8838\n",
      "\n",
      "Epoch 00107: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 107/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0402 - accuracy: 0.9859 - val_loss: 0.6294 - val_accuracy: 0.8830\n",
      "\n",
      "Epoch 00108: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 108/300\n",
      "196/195 [==============================] - 101s 514ms/step - loss: 0.0387 - accuracy: 0.9863 - val_loss: 0.5116 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00109: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 109/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0376 - accuracy: 0.9865 - val_loss: 0.5160 - val_accuracy: 0.8925\n",
      "\n",
      "Epoch 00110: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 110/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0348 - accuracy: 0.9883 - val_loss: 0.4987 - val_accuracy: 0.8930\n",
      "\n",
      "Epoch 00111: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 111/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0375 - accuracy: 0.9877 - val_loss: 0.5189 - val_accuracy: 0.8947\n",
      "\n",
      "Epoch 00112: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 112/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0341 - accuracy: 0.9880 - val_loss: 0.6330 - val_accuracy: 0.8731\n",
      "\n",
      "Epoch 00113: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 113/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0350 - accuracy: 0.9882 - val_loss: 0.6191 - val_accuracy: 0.8941\n",
      "\n",
      "Epoch 00114: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 114/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0345 - accuracy: 0.9878 - val_loss: 0.6419 - val_accuracy: 0.8768\n",
      "\n",
      "Epoch 00115: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 115/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0353 - accuracy: 0.9881 - val_loss: 0.4999 - val_accuracy: 0.8939\n",
      "\n",
      "Epoch 00116: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 116/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0359 - accuracy: 0.9876 - val_loss: 0.5132 - val_accuracy: 0.8988\n",
      "\n",
      "Epoch 00117: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 117/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0365 - accuracy: 0.9875 - val_loss: 0.6647 - val_accuracy: 0.8795\n",
      "\n",
      "Epoch 00118: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 118/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0332 - accuracy: 0.9887 - val_loss: 0.5751 - val_accuracy: 0.8844\n",
      "\n",
      "Epoch 00119: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 119/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0341 - accuracy: 0.9874 - val_loss: 0.4365 - val_accuracy: 0.9067\n",
      "\n",
      "Epoch 00120: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 120/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0341 - accuracy: 0.9882 - val_loss: 0.6984 - val_accuracy: 0.8729\n",
      "\n",
      "Epoch 00121: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 121/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0315 - accuracy: 0.9889 - val_loss: 0.5610 - val_accuracy: 0.8904\n",
      "\n",
      "Epoch 00122: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 122/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0326 - accuracy: 0.9887 - val_loss: 0.5674 - val_accuracy: 0.8895\n",
      "\n",
      "Epoch 00123: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 123/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0347 - accuracy: 0.9878 - val_loss: 0.5380 - val_accuracy: 0.8969\n",
      "\n",
      "Epoch 00124: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 124/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0304 - accuracy: 0.9893 - val_loss: 0.6794 - val_accuracy: 0.8773\n",
      "\n",
      "Epoch 00125: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 125/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0315 - accuracy: 0.9890 - val_loss: 0.5990 - val_accuracy: 0.8772\n",
      "\n",
      "Epoch 00126: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 126/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0299 - accuracy: 0.9895 - val_loss: 0.4417 - val_accuracy: 0.9055\n",
      "\n",
      "Epoch 00127: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 127/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0334 - accuracy: 0.9880 - val_loss: 0.6050 - val_accuracy: 0.8852\n",
      "\n",
      "Epoch 00128: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 128/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0302 - accuracy: 0.9900 - val_loss: 0.5902 - val_accuracy: 0.8878\n",
      "\n",
      "Epoch 00129: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 129/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0293 - accuracy: 0.9896 - val_loss: 0.5152 - val_accuracy: 0.9041\n",
      "\n",
      "Epoch 00130: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 130/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0328 - accuracy: 0.9884 - val_loss: 0.5628 - val_accuracy: 0.8923\n",
      "\n",
      "Epoch 00131: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 131/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0317 - accuracy: 0.9890 - val_loss: 0.5493 - val_accuracy: 0.8935\n",
      "\n",
      "Epoch 00132: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 132/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0281 - accuracy: 0.9905 - val_loss: 0.5303 - val_accuracy: 0.8887\n",
      "\n",
      "Epoch 00133: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 133/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0273 - accuracy: 0.9907 - val_loss: 0.4362 - val_accuracy: 0.9104\n",
      "\n",
      "Epoch 00134: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 134/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0246 - accuracy: 0.9912 - val_loss: 0.4818 - val_accuracy: 0.9031\n",
      "\n",
      "Epoch 00135: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 135/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0251 - accuracy: 0.9916 - val_loss: 0.6996 - val_accuracy: 0.8745\n",
      "\n",
      "Epoch 00136: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 136/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0264 - accuracy: 0.9911 - val_loss: 0.4794 - val_accuracy: 0.9020\n",
      "\n",
      "Epoch 00137: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 137/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0260 - accuracy: 0.9911 - val_loss: 0.4789 - val_accuracy: 0.9063\n",
      "\n",
      "Epoch 00138: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 138/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0268 - accuracy: 0.9906 - val_loss: 0.4762 - val_accuracy: 0.9021\n",
      "\n",
      "Epoch 00139: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 139/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0288 - accuracy: 0.9903 - val_loss: 0.4827 - val_accuracy: 0.8968\n",
      "\n",
      "Epoch 00140: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 140/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.5409 - val_accuracy: 0.8983\n",
      "\n",
      "Epoch 00141: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 141/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0275 - accuracy: 0.9905 - val_loss: 0.6261 - val_accuracy: 0.8853\n",
      "\n",
      "Epoch 00142: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 142/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0300 - accuracy: 0.9902 - val_loss: 0.4812 - val_accuracy: 0.9005\n",
      "\n",
      "Epoch 00143: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 143/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0269 - accuracy: 0.9904 - val_loss: 0.7035 - val_accuracy: 0.8781\n",
      "\n",
      "Epoch 00144: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 144/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0251 - accuracy: 0.9909 - val_loss: 0.5309 - val_accuracy: 0.8911\n",
      "\n",
      "Epoch 00145: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 145/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0251 - accuracy: 0.9917 - val_loss: 0.5547 - val_accuracy: 0.8957\n",
      "\n",
      "Epoch 00146: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 146/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0255 - accuracy: 0.9913 - val_loss: 0.5560 - val_accuracy: 0.8918\n",
      "\n",
      "Epoch 00147: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 147/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0208 - accuracy: 0.9927 - val_loss: 0.5646 - val_accuracy: 0.8921\n",
      "\n",
      "Epoch 00148: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 148/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0265 - accuracy: 0.9905 - val_loss: 0.5413 - val_accuracy: 0.8963\n",
      "\n",
      "Epoch 00149: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 149/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0277 - accuracy: 0.9905 - val_loss: 0.6097 - val_accuracy: 0.8893\n",
      "\n",
      "Epoch 00150: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 150/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0245 - accuracy: 0.9912 - val_loss: 0.4929 - val_accuracy: 0.9030\n",
      "\n",
      "Epoch 00151: LearningRateScheduler reducing learning rate to 0.1.\n",
      "Epoch 151/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0246 - accuracy: 0.9916 - val_loss: 0.5166 - val_accuracy: 0.8993\n",
      "\n",
      "Epoch 00152: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 152/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0188 - accuracy: 0.9938 - val_loss: 0.4082 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00153: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 153/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0113 - accuracy: 0.9964 - val_loss: 0.4081 - val_accuracy: 0.9177\n",
      "\n",
      "Epoch 00154: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 154/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0099 - accuracy: 0.9970 - val_loss: 0.4011 - val_accuracy: 0.9181\n",
      "\n",
      "Epoch 00155: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 155/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.4057 - val_accuracy: 0.9169\n",
      "\n",
      "Epoch 00156: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 156/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.3952 - val_accuracy: 0.9195\n",
      "\n",
      "Epoch 00157: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 157/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0079 - accuracy: 0.9975 - val_loss: 0.3946 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00158: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 158/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0078 - accuracy: 0.9975 - val_loss: 0.3971 - val_accuracy: 0.9192\n",
      "\n",
      "Epoch 00159: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 159/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.3937 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00160: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 160/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0076 - accuracy: 0.9977 - val_loss: 0.4048 - val_accuracy: 0.9178\n",
      "\n",
      "Epoch 00161: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 161/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.3895 - val_accuracy: 0.9197\n",
      "\n",
      "Epoch 00162: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 162/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0072 - accuracy: 0.9978 - val_loss: 0.3947 - val_accuracy: 0.9196\n",
      "\n",
      "Epoch 00163: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 163/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.3908 - val_accuracy: 0.9214\n",
      "\n",
      "Epoch 00164: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 164/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0069 - accuracy: 0.9980 - val_loss: 0.3943 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00165: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 165/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 0.3935 - val_accuracy: 0.9208\n",
      "\n",
      "Epoch 00166: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 166/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0065 - accuracy: 0.9980 - val_loss: 0.3910 - val_accuracy: 0.9204\n",
      "\n",
      "Epoch 00167: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 167/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 0.4040 - val_accuracy: 0.9183\n",
      "\n",
      "Epoch 00168: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 168/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.3998 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00169: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 169/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0062 - accuracy: 0.9983 - val_loss: 0.3931 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00170: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 170/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.3921 - val_accuracy: 0.9209\n",
      "\n",
      "Epoch 00171: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 171/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0053 - accuracy: 0.9984 - val_loss: 0.4034 - val_accuracy: 0.9197\n",
      "\n",
      "Epoch 00172: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 172/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0061 - accuracy: 0.9984 - val_loss: 0.3976 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 00173: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 173/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0050 - accuracy: 0.9987 - val_loss: 0.3946 - val_accuracy: 0.9217\n",
      "\n",
      "Epoch 00174: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 174/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.3978 - val_accuracy: 0.9220\n",
      "\n",
      "Epoch 00175: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 175/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0060 - accuracy: 0.9981 - val_loss: 0.3955 - val_accuracy: 0.9208\n",
      "\n",
      "Epoch 00176: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 176/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0058 - accuracy: 0.9982 - val_loss: 0.4072 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00177: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 177/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0056 - accuracy: 0.9983 - val_loss: 0.3923 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00178: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 178/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3926 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00179: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 179/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.3891 - val_accuracy: 0.9225\n",
      "\n",
      "Epoch 00180: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 180/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.3972 - val_accuracy: 0.9208\n",
      "\n",
      "Epoch 00181: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 181/300\n",
      "196/195 [==============================] - 99s 507ms/step - loss: 0.0048 - accuracy: 0.9986 - val_loss: 0.4026 - val_accuracy: 0.9205\n",
      "\n",
      "Epoch 00182: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 182/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.4015 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00183: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 183/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0044 - accuracy: 0.9987 - val_loss: 0.4016 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 00184: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 184/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0044 - accuracy: 0.9989 - val_loss: 0.4015 - val_accuracy: 0.9198\n",
      "\n",
      "Epoch 00185: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 185/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0047 - accuracy: 0.9987 - val_loss: 0.4081 - val_accuracy: 0.9191\n",
      "\n",
      "Epoch 00186: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 186/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.4077 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00187: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 187/300\n",
      "196/195 [==============================] - 100s 510ms/step - loss: 0.0046 - accuracy: 0.9987 - val_loss: 0.4049 - val_accuracy: 0.9203\n",
      "\n",
      "Epoch 00188: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 188/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0045 - accuracy: 0.9987 - val_loss: 0.4067 - val_accuracy: 0.9207\n",
      "\n",
      "Epoch 00189: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 189/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.4049 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00190: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 190/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0043 - accuracy: 0.9987 - val_loss: 0.4073 - val_accuracy: 0.9199\n",
      "\n",
      "Epoch 00191: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 191/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0046 - accuracy: 0.9989 - val_loss: 0.4077 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00192: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 192/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.4161 - val_accuracy: 0.9187\n",
      "\n",
      "Epoch 00193: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 193/300\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0045 - accuracy: 0.9986 - val_loss: 0.4086 - val_accuracy: 0.9202\n",
      "\n",
      "Epoch 00194: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 194/300\n",
      "196/195 [==============================] - 99s 508ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.4058 - val_accuracy: 0.9201\n",
      "\n",
      "Epoch 00195: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 195/300\n",
      "196/195 [==============================] - 99s 506ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.4049 - val_accuracy: 0.9213\n",
      "\n",
      "Epoch 00196: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 196/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.4026 - val_accuracy: 0.9210\n",
      "\n",
      "Epoch 00197: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 197/300\n",
      "196/195 [==============================] - 100s 509ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.4009 - val_accuracy: 0.9217\n",
      "\n",
      "Epoch 00198: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 198/300\n",
      "196/195 [==============================] - 100s 508ms/step - loss: 0.0043 - accuracy: 0.9989 - val_loss: 0.4171 - val_accuracy: 0.9200\n",
      "\n",
      "Epoch 00199: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 199/300\n",
      "195/195 [============================>.] - ETA: 0s - loss: 0.0040 - accuracy: 0.9989Restoring model weights from the end of the best epoch.\n",
      "196/195 [==============================] - 100s 511ms/step - loss: 0.0040 - accuracy: 0.9989 - val_loss: 0.4126 - val_accuracy: 0.9195\n",
      "Epoch 00199: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE),epochs=EPOCHS,\n",
    "                              validation_data=(x_test, y_test),steps_per_epoch=len(x_train)/BATCH_SIZE,\n",
    "                              callbacks=[lrate_sched, early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3hc1Zn48e87o1GvluUmF7lhXADbGGNqIJSYXgOEkrBJ1skm2SS7WTYhhZTdTdnkl0JIQmgLSQgdggOGAAm92hjbuDdsLDfJRb2O5vz+eO94RqNiyfbVSJr38zx6ZubeO/eeuTM672n3XHHOYYwxJnUFkp0AY4wxyWWBwBhjUpwFAmOMSXEWCIwxJsVZIDDGmBRngcAYY1KcBQJjekhE7hWR/+7htltE5OzD3Y8xfcECgTHGpDgLBMYYk+IsEJhBxWuSuUlEVohIvYjcLSLDReQZEakVkRdEpChu+4tFZJWIVInISyIyNW7dLBFZ6r3vISAz4VgXisgy771viMixh5jmfxaRjSKyT0QWisgob7mIyC9EpEJEakTkfRGZ4a07X0RWe2nbLiL/cUgnzBgsEJjB6QrgHOAo4CLgGeCbQAn6m/8ygIgcBTwAfNVbtwj4q4iki0g68Bfgj8AQ4BFvv3jvnQXcA3wOKAZ+DywUkYzeJFREPgr8CLgKGAlsBR70Vp8LnO59jgJvm73euruBzznn8oAZwD96c1xj4lkgMIPRr51zu51z24FXgbedc+8555qAJ4BZ3nZXA0875553zrUCPwOygJOBeUAI+KVzrtU59yiwOO4YC4DfO+feds61OefuA5q99/XGdcA9zrmlzrlm4GbgJBEpA1qBPOBoQJxza5xzO733tQLTRCTfObffObe0l8c15gALBGYw2h33vLGT17ne81FoCRwA51wE2AaUeuu2u/azMm6Nez4O+JrXLFQlIlXAGO99vZGYhjq01F/qnPsHcBvwG6BCRO4QkXxv0yuA84GtIvKyiJzUy+Mac4AFApPKdqAZOqBt8mhmvh3YCZR6y6LGxj3fBvyPc64w7i/bOffAYaYhB21q2g7gnLvVOXc8MA1tIrrJW77YOXcJMAxtwnq4l8c15gALBCaVPQxcICJniUgI+BravPMG8CYQBr4sIiERuRyYG/feO4HPi8iJXqdujohcICJ5vUzDA8A/ichMr3/hh2hT1hYROcHbfwioB5qAiNeHcZ2IFHhNWjVA5DDOg0lxFghMynLOrQOuB34N7EE7li9yzrU451qAy4EbgX1of8Ljce9dAvwz2nSzH9jobdvbNLwAfAd4DK2FTASu8VbnowFnP9p8tBf4qbfuBmCLiNQAn0f7Gow5JGI3pjHGmNRmNQJjjElxFgiMMSbFWSAwxpgUZ4HAGGNSXFqyE9BbQ4cOdWVlZclOhjHGDCjvvvvuHudcSWfrBlwgKCsrY8mSJclOhjHGDCgisrWrddY0ZIwxKc4CgTHGpDgLBMYYk+IGXB9BZ1pbWykvL6epqSnZSfFdZmYmo0ePJhQKJTspxphBYlAEgvLycvLy8igrK6P9ZJGDi3OOvXv3Ul5ezvjx45OdHGPMIDEomoaampooLi4e1EEAQEQoLi5OiZqPMabvDIpAAAz6IBCVKp/TGNN3Bk0gOKjWRqjZAW2tyU6JMcb0K6kTCMJNULcbIuEjvuuqqip++9vf9vp9559/PlVVVUc8PcYY0xupEwiINqkc+fsvdBUIwuHug86iRYsoLCw84ukxxpjeGBSjhnrFh/vwfOMb32DTpk3MnDmTUChEZmYmRUVFrF27lvXr13PppZeybds2mpqa+MpXvsKCBQuA2HQZdXV1nHfeeZx66qm88cYblJaW8uSTT5KVlXXkE2uMMQkGXSD4/l9XsXpHTccVkbA2D4XqQYK92ue0Ufl896LpXa7/8Y9/zMqVK1m2bBkvvfQSF1xwAStXrjwwxPOee+5hyJAhNDY2csIJJ3DFFVdQXFzcbh8bNmzggQce4M477+Sqq67iscce4/rrr+9VOo0x5lAMukDQtb4bbTN37tx24/xvvfVWnnjiCQC2bdvGhg0bOgSC8ePHM3PmTACOP/54tmzZ0mfpNcaktkEXCLosuTfVwL5NUDwZMnJ9TUNOTs6B5y+99BIvvPACb775JtnZ2ZxxxhmdXgeQkZFx4HkwGKSxsdHXNBpjTFTqdBaLf53FeXl51NbWdrquurqaoqIisrOzWbt2LW+99dYRP74xxhyOQVcjSIbi4mJOOeUUZsyYQVZWFsOHDz+wbv78+dx+++1MnTqVKVOmMG/evCSm1BhjOhLnfBhG46M5c+a4xBvTrFmzhqlTp3b/xuY62LsBhkyEzHwfU+i/Hn1eY4yJIyLvOufmdLYuBZuGjDHGxPMtEIhIpoi8IyLLRWSViHy/k21uFJFKEVnm/X3Wr/TEDKwakDHG+M3PPoJm4KPOuToRCQGvicgzzrnE3tKHnHNf8jEdHq9GMMCawowxxm++BQKnnQ913suQ95e8XNhahowxplO+9hGISFBElgEVwPPOubc72ewKEVkhIo+KyJgu9rNARJaIyJLKyspDTY33aDUCY4yJ52sgcM61OedmAqOBuSIyI2GTvwJlzrljgeeB+7rYzx3OuTnOuTklJSWHmBprGjLGmM70yagh51wV8CIwP2H5Xudcs/fyLuD4vkjPkXao01AD/PKXv6ShoeEIp8gYY3rOz1FDJSJS6D3PAs4B1iZsMzLu5cXAGr/S4+eVxRYIjDEDmZ+jhkYC94lIEA04DzvnnhKRHwBLnHMLgS+LyMVAGNgH3OhjenwTPw31Oeecw7Bhw3j44Ydpbm7msssu4/vf/z719fVcddVVlJeX09bWxne+8x12797Njh07OPPMMxk6dCgvvvhisj+KMSYF+TlqaAUwq5Plt8Q9vxm4+Yge+JlvwK73O0lQBFrrIS0TAqHe7XPEMXDej7tcHT8N9XPPPcejjz7KO++8g3OOiy++mFdeeYXKykpGjRrF008/DegcRAUFBfz85z/nxRdfZOjQob1LkzHGHCEpdGVx9Im/ncXPPfcczz33HLNmzWL27NmsXbuWDRs2cMwxx/D888/z9a9/nVdffZWCggJf02GMMT01+Cad66rk3tYKu1dCwWjIOdSRRwfnnOPmm2/mc5/7XId1S5cuZdGiRXz729/mrLPO4pZbbulkD8YY07dSp0bg4/DR+GmoP/axj3HPPfdQV6fX0m3fvp2Kigp27NhBdnY2119/PTfddBNLly7t8F5jjEmGwVcj6IqPVxbHT0N93nnnce2113LSSScBkJuby5/+9Cc2btzITTfdRCAQIBQK8bvf/Q6ABQsWMH/+fEaNGmWdxcaYpEidaagjbbBrBeSPgtzh3W/bz9k01MaY3rJpqIFY01ByU2GMMf1N6gSCPho1ZIwxA82gCQQHb+IaHJPODbSmPGNM/zcoAkFmZiZ79+7tWSY5gPNR5xx79+4lMzMz2Ukxxgwig2LU0OjRoykvL+egU1RXVUJmE2RW9U3CfJCZmcno0aOTnQxjzCAyKAJBKBRi/PjxB9/wB6fByV+Cs7/nd5KMMWbAGBRNQz0WCOowUmOMMQekViCQoE4+Z4wx5oDUCgSBNKsRGGNMghQLBAFwFgiMMSZeagUCCUIknOxUGGNMv5JagcA6i40xpoPUCgQStKYhY4xJ4OfN6zNF5B0RWS4iq0Tk+51skyEiD4nIRhF5W0TK/EoP4NUIbNSQMcbE87NG0Ax81Dl3HDATmC8i8xK2+Qyw3zk3CfgF8BMf0wNincXGGJPIt0DgVJ33MuT9Jc70cwlwn/f8UeAsEfHvFjI2fNQYYzrwtY9ARIIisgyoAJ53zr2dsEkpsA3AORcGqoHiTvazQESWiMiSg84n1J2A9REYY0wiXwOBc67NOTcTGA3MFZEZh7ifO5xzc5xzc0pKDuPG8zZ81BhjOuiTUUPOuSrgRWB+wqrtwBgAEUkDCoC9viXEOouNMaYDP0cNlYhIofc8CzgHWJuw2ULgU97zK4F/OD/vvGKdxcYY04Gf01CPBO4TkSAacB52zj0lIj8AljjnFgJ3A38UkY3APuAaH9NjF5QZY0wnfAsEzrkVwKxOlt8S97wJ+LhfaejALigzxpgOUuvKYhs+aowxHaRYILD7ERhjTKLUCgQSsBqBMcYkSK1AELDrCIwxJlFqBQLrLDbGmA5SKxDY8FFjjOkgtQKB1QiMMaaD1AoENsWEMcZ0kHqBwGoExhjTTmoFArE+AmOMSZRagcBqBMYY00FqBQK7H4ExxnSQWoHAOouNMaaD1AoEdj8CY4zpILUCgc0+aowxHaRYILDOYmOMSZRagcCGjxpjTAd+3rN4jIi8KCKrRWSViHylk23OEJFqEVnm/d3S2b6OGLsfgTHGdODnPYvDwNecc0tFJA94V0Sed86tTtjuVefchT6mI0YCNnzUGGMS+FYjcM7tdM4t9Z7XAmuAUr+O1yM2+6gxxnTQJ30EIlKG3sj+7U5WnyQiy0XkGRGZ3sX7F4jIEhFZUllZeRgJsc5iY4xJ5HsgEJFc4DHgq865moTVS4FxzrnjgF8Df+lsH865O5xzc5xzc0pKSg49MVYjMMaYDnwNBCISQoPA/c65xxPXO+dqnHN13vNFQEhEhvqWoEAa4MA53w5hjDEDjZ+jhgS4G1jjnPt5F9uM8LZDROZ66dnrV5qQoD5arcAYYw7wc9TQKcANwPsissxb9k1gLIBz7nbgSuBfRCQMNALXOOdjcT3gxT3Xhr8f3RhjBg7fckPn3GuAHGSb24Db/EpDB1YjMMaYDlLryuJANBDYtQTGGBOVWoEgWiOwIaTGGHNAagWCAzUCm2bCGGOiUjMQWI3AGGMOSK1AYJ3FxhjTQWoFAqsRGGNMB6kVCKxGYIwxHaRWILDho8YY00FqBYIDw0dt1JAxxkSlViCITjFhTUPGGHNAigUCb0YN6yw2xpgDUisQWGexMcZ0kFqBwIaPGmNMB6kVCMSmmDDGmESpFQja3Y/AGGMMpFogELuOwBhjEqVWIAhYZ7ExxiTy857FY0TkRRFZLSKrROQrnWwjInKriGwUkRUiMtuv9AA2fNQYYzrh5417w8DXnHNLRSQPeFdEnnfOrY7b5jxgsvd3IvA779EfNnzUGGM68K1G4Jzb6Zxb6j2vBdYApQmbXQL8wam3gEIRGelXmmLDR23UkDHGRPVJH4GIlAGzgLcTVpUC2+Jel9MxWBzBhNgUE8YYk6hHgUBEviIi+V6b/t0islREzu3he3OBx4CvOudqDiWRIrJARJaIyJLKyspD2YWyC8qMMaaDntYIPu1l4ucCRcANwI8P9iYRCaFB4H7n3OOdbLIdGBP3erS3rB3n3B3OuTnOuTklJSU9THJnCbI+AmOMSdTTQCDe4/nAH51zq+KWdf4GEQHuBtY4537exWYLgU96NY15QLVzbmcP09R7dj8CY4zpoKejht4VkeeA8cDN3iigg/W4noLWHN4XkWXesm8CYwGcc7cDi9DgshFoAP6pd8nvJbsfgTHGdNDTQPAZYCaw2TnXICJDOEim7Zx7jYPUGpxzDvhiD9Nw+OyCMmOM6aCnTUMnAeucc1Uicj3wbaDav2T5xDqLjTGmg54Ggt8BDSJyHPA1YBPwB99S5RfrLDbGmA56GgjCXjPOJcBtzrnfAHn+JcsnViMwxpgOetpHUCsiN6Odv6eJSAAI+Zcsn1iNwBhjOuhpjeBqoBm9nmAXOt7/p76lyi/WWWyMMR30KBB4mf/9QIGIXAg0OecGYB+B3ZjGGGMS9XSKiauAd4CPA1cBb4vIlX4mzBfRaaitRmCMMQf0tI/gW8AJzrkKABEpAV4AHvUrYb6wzmJjjOmgp30EgWgQ8OztxXv7D+ssNsaYDnpaI3hWRP4GPOC9vhqdHmJgsRqBMcZ00KNA4Jy7SUSuQOcPArjDOfeEf8nyyYEagc01ZIwxUT2+VaVz7jF0SumBK2CjhowxJlG3gUBEagHX2Sp0zrh8X1LlJwnaNNTGGBOn20DgnBt400gcTCDNOouNMSbOwBv5c7gCQWsaMsaYOKkXCCRoncXGGBMn9QJBIGA1AmOMiZN6gUCC1kdgjDFxfAsEInKPiFSIyMou1p8hItUissz7u8WvtLRjfQTGGNNOj68jOAT3ArfR/Z3MXnXOXehjGjqy4aPGGNOObzUC59wrwD6/9n/IAmnWWWyMMXGS3UdwkogsF5FnRGR6VxuJyAIRWSIiSyorKw/viNZZbIwx7SQzECwFxjnnjgN+Dfylqw2dc3c45+Y45+aUlJQc3lGts9gYY9pJWiBwztU45+q854uAkIgM9f3A1llsjDHtJC0QiMgIERHv+VwvLXv9P7DVCIwxJp5vo4ZE5AHgDGCoiJQD3wVCAM6524ErgX8RkTDQCFzjnOtsgrsjy2oExhjTjm+BwDn3iYOsvw0dXtonapta2bq3gWkECNioIWOMOSDZo4b6zEvrKrnw16/R4gIQaU12cowxpt9ImUBQkBUCoCUtD5qqk5waY4zpP1ImEOR7gaAxLR8a+t91bsYYkywpEwiiNYL6YD40WiAwxpiolAsENZIPjfttmgljjPGkTCDIz9QBUtXkgotAs/UTGGMMpFAgSAsGyM1IY5/L1QVd9RPsWAZtNqrIGJM6UiYQgDYP7WnzAkHj/o4b1FXAnWfC+4/0bcKMMSaJUioQ5GeFqAhn64vOagQN+7TZaN/mvk2YMcYkUUoFgoKsNHa15uiLhk6mNWqp18eanX2XKGOMSbKUCgT5mSF2tGTpi86GkLbU6WPtjr5LlDHGJFlKBYKCrBA7GkMggc6bhqxGYIxJQSkXCKqa2iCrqIsaQTQQWI3AGJM6Ui4QNLa24bKGdFEj8JqGmqtjQcEYYwa51AoE2Xp1cTijsPsaAVjzkDEmZaRWIIjOQJpeCA2dXEfQLhBs76NUGWNMcqVUIIjNQFrQ/aghgFqrERhjUoNvgUBE7hGRChFZ2cV6EZFbRWSjiKwQkdl+pSUqWiNoCBZ0PWoo5F1wZh3GxpgU4WeN4F5gfjfrzwMme38LgN/5mBYgFghqA3kQboTWxvYbtNRDzlDILOhfgaB8CfTB7ZyNManJt0DgnHsF6G7i/0uAPzj1FlAoIiP9Sg/oBWUANZKnCxJrBS11kJ4LeaP6T9PQjmVw11mw5bVkp8QYM0gls4+gFNgW97rcW9aBiCwQkSUisqSysvKQDxitEeyPzkCa2E/QUg/pOZA/sv/UCPZv0cf6iqQmwxgzeA2IzmLn3B3OuTnOuTklJSWHvJ/0tABZoSA7GaoLdiV0X0QDQd6o/jNqqHaXPjbXJjcdxphBK5mBYDswJu71aG+ZrwqyQqyTiVA4Dpb/uf3KlnptGiqZAnW7oe7Qax9HTLSJqrmu++2MMeYQJTMQLAQ+6Y0emgdUO+d8b5gvyAqxv6kNZl4HH7wCVR/GVrbUaY1g9Bx9vX2J38k5uLrd+mg1AmOMT/wcPvoA8CYwRUTKReQzIvJ5Efm8t8kiYDOwEbgT+IJfaYlXWpTFtn0NcNw1umD5g7GVrQ0aCEbOBAnqaJ1ki9YIWqxGYIzxR5pfO3bOfeIg6x3wRb+O35XJw3J5beMewvljSCs7DVY8DB/5T10Z7SNIz4bh0/pHjaA2WiOoSW46emPHMg1gU85LdkqMMT0wIDqLj6SJw3JpCUfYtr8Rpl0CezdA5XqItHk1Am9E0egTYPtSiESSm+ADfQQDqGno9V/B019LdiqMMT2UcoFg8jDN6DfsroUp5+vCtU9pEACtEQCUztFS+N4NSUilp7URmqr0+UDqLG6q0jvA2UVwxgwIKRcIJnmBYGNlHRSUwqjZsPbp2IRz0UAQ7TAuX5yEVHqiHcUwsGoETTUQbrKpvI0ZIFIuEORlhhhZkMnG3V4Je+qF2hewxyv5R5uGiidDRkFyO4yj1xCkZQ2szuJof0Zn94U2xvQ7KRcIQGsFGyq8jHXyufq4/ll9jNYIAgEonRXrMH7nTtj1ft8mNBoIiicNrM7iJgsExgwkKRkIJg/LY2NFHZGIg6FH6T2Mdy7XldFAANpPsHu1diYv+g9YfFffJjQaCIZOHlh9BAdqBN1NNWWM6S9SMhBMGpZLY2sb26saIS0DisriAkFubMPRc8C1wUs/0tfReX/6Su1OCISgcKz2EQyEzte21ljHu9UIjBkQUjIQHD1SZx9dUV6tC4onx0qxiTUCgFWP62NfB4K63ZA3AjLzIdIK4ea+PX5vtIU1CMR3alsgMGZASMlAcGxpAXmZaby6wZtLaOjk2Mr4QJBbonMSAQTToWqbZnjxNr7gX4dyzXbIHQ7p3rTZ/bnDeOG/wkM3QFN1bFnDnuSlxxjTYykZCNKCAU6ZOJRX1lfinEsIBLntN44OIz32Km0mqimPrXMOnvg8PH/LkU9kpA12LIfh0yHDCwT9ucN45zKoWJ0QCAZ5jaAtDG/fkfyaWmMVrHrCn3031ehAiZ42S0YiejHhzhX+pMf4IiUDAcDpR5Wwo7qJTZV12mEcFV8jADj2ajjqPDjm4/o6vnloz3qor4Tdq458+33FamiuhnEnQ4YXnPpzh3F1OdTvaR+s+ioQPPxJWL3w8Pez5q/wwCd6/l1ufhGeuQnW/+3wj304lj8Ij9yogxritbXCe/fr46Fa9bgOlIj2oR1M9TYdVPH+w4d+TNPnUjYQnDZZ70nwyvo92kcAEEjTJqB4R30Mrn0QhkzU1/s+iK3b8qo+NlXpjWzCze1Lh21heOF7h3aTm61v6uO4k+NqBN1cVFa7q2OzVV9pqtYA0FofmxspPa9vRg3V74HVT8L7jxz+vtYugnWLvHPZCvs2d7/9bu9+FgfbzrkjV1BwDp7/rs7nFFXt3d8pcW6sdc/Ak1/Q+bQOVfT33tP+seh2FWsP/Zimz6VsIBgzJJsJQ3N4cV2Fd5/iQq0NiHT+hvxROoIn/h9iy2uAt/3uVfDwp7SdPGrHe/DaL7TaHm6G356kmU1PbH0d8kfriKGDBYLWJrjtBHjuW7FlzsXG8/utKu5Gc/s26eOQsr6pEVSu08edy7rfrif2e5lexWptDvnNPGjcrwFt8V3aXBdv96r27+tMuAV+Pg2W3HNoadq+FD54Nfa6ehu8/kt47edxy7zmyu3vJrzXe73iQQ5Z9Pfe00BQtVUfK9Yc+jFNn0vZQABw3jEjeH3jHnbWNGk/QWL/QLxAUDPl/Vugfq8202x5LTbD5tbXYePzWkuIlsx3ee2kezfB3o2awWzopBnhrd+1/2d3Dj58U2sD0L6zuLVJ9xd/05ydy7REvuSeWKaw8jH42WSoPsL3+lnxSMf7J1fH9Zvs3aiPQyb0USDwSp5VHx5+DSRa+q1Yo99nW7NeR7Lsz9ruvfbp9tvvXt3+fZ3Z9T7U7mjffPT+o/DAtdDS0Pl7nv4a/Oo4eOrf4a6z4aHrYjWKaNv7hudj74/WOBMDwY739PGDVw/9d9CTQBBugee+o7+D6HY15X1XEDGHLaUDwVVzxhBx8MiScpj4Ub0PQXeKyvSf67Y58POp2j8w5XwoGAvv/h9EwjqGfo9XSo1eibxvc2wKi8Srk+v3wN++Ca/8NLZs32YdOjruJH0drRHs3QQ/nQi/ng13fjSWOXz4lj66CLzqlRRXPqbz/Wz6R6/PS5eaauDJL2o7+o5l8Odr4B//HWuaiKYRoGi8ZszR2Vs3v9w+o3ZOOzl7Y9tizXTi7YlrFz9YrSDcrJlwa1Ns2ZbXtPTfUg913gV8FWu0JA5a6o+2j7/5m7h9tcS+5+5qBNve1sfyxfqZX/kZPPYZWPc0bH2j4/aRiH539Xtgyd2QX6pNb9Hmp2haWht0xBrEbqu6a2Xss0Ui+h1NOANwsaazHe9pgaGrIJSoJ4Fgw3Pwxq2w4qH220Vra535+w9g6R96lgbju5QOBOOKczhlUjEPLd5G5CM3wyf+3P0bisq06tvWAqXHQygbJp6pI3uaqmP9C9FM5EAg2BSbxXT36vZNDOv/phn4h2/pbKOgpT2AstP1MdpZvPEFrRVMOgeqP4xlQNve0T6M2Z/Uf67K9bDpRV23+cVDOjedWv+slpLDzXDHGbD+Ge2MbFcj2KTnJW+EjrJqqtIS8x8uhrdvj233/iPaZNLTUvzuVXD32ZrZxKtcp7UPaN9unqgtDI9+WjPhaLNKWxie/BI8+41Ym7YE9JzVeqXs3Ss18w2mw7a3oNwrde/doIG/aLx+/sQAFVX+jj427tNa3ov/A0dfqP1RW1/ruH3FKm2OOv9n8JXlcM2fvM/mle53rdApR7KGwJqF+hlqd0HJ0XqtSXzho7kaZlyh18Os+asu/8f/wFP/BrfOPPiUKY37Y7PfRpt8OrPyMS+Ny2D/Vq05A1R20Ty0d5MWWN78bffHN30mpQMBwNUnjGV7VSPPr9l98I2LJ+njBf8PPvkXuLkcCkZrIACYdqlOVLf9Xc3sd6/SO51Vl8fak8ON7TsX1y0CRDPYaMl+1eMwbDoM9Y4XytFtdizV/Z35TV3+4Vtaytz2Now5EU77mvZxPHitHqdwrJbEu7qnQu0uuG2ulrR7YuXj2m/x8Xt139Mu1Qxz29uQN0q3aa6GjHzILtbXDftg5aP6vGJ1bF9b39DO5R3v6WdYvRD+74L2NaN4657Rx8qETsjKdfrZi8q6rxG88F2dbrxwnJbs6/fA6r9oMI2EYxcNjpkXK2FnFmpJfs96mLtAP9eSu3Vd9PuceqEG8vhaUbxt78DwGfr82Zt127Nu0Vlvt7zecfsPXtHH8afrZxo2DdIyY4Fg5wothBx9Pqx7VtPq2mDqxbr+rd9qJhsdyDBqNoydpwGtLazpHnuy1hZf+6Vus+M9PR+JoqX7oUdp01tiHwloTSo6T9fOZfqe8R/RiRLj+wka9ul3vO8Db6oWp4GiP9wX3FggmD99BJOG5fJfT62msaWTH3q82TfAJ5+M3eYyENTHaCCYMt+bqO5dLfWEG2H8afrPv+lFyNaRSgdKYq2N2nRz3Ce0hLj5JQ0a296GGZfFjhsIaP9FJKwZw8iZmkltfUODSsMeGHuiBqXZn9LSakYBnH6TrinmkOAAAB2uSURBVNu9svOx7svu1+aNd+89+IlqrNIayfRLNRP66go49au67sM3oXiiZpSgV0JnD9HnDXu0OQbajySJZqS7VmgG/fAN8OEbsOTezkfYbHhOH+Pb45tqNBANPUrPSTSzTOSc1iSmXQLXPaLNKn/5Arz041jpNdp0crR3j4pAGky/zAteDspO1dpftC9n9yodPDDpHH29Z4NeUxBt7lr3rAbYmu0w63r9PnYugxHHQskUKDtFA3viVN0fvKq1u4JSfR0MwYhjtJZZV6mfd8SxMOFMaKmNNQ+NnqNBbtXj8Pfvw6KbNDMuOVoDUbhJRxXV7tB+rWOv1lrC+ue0dveL6fDM19tfBxINBBPO0N9eTSf9DOuf1fM5+WNeP80eGDIeSo7Sz3LnWfCjsfC/E/Q7vv1UrbUOnaLv39pJMDR9ztdAICLzRWSdiGwUkW90sv5GEakUkWXe32f9TE9n0tMC/NclMyjf38hvXtzY/cYZeV6ba4KjL9BawtSLtbRWsTrWJDDtUn1sqtJ/wECalhLvuxh+f7r+Ex37cRg9VwPBqr/o9tMv73hsgFHHaWAYO09rBNE26DEn6uNp/w7BDDjqXJh0ti7789Xww1LY8IJmlvecp4/v3a/r1z7VddNG1NqntekhPl3DZ2hmA1AwRkdfQfsaweaXtBSfN1KDVrhZayjR2sHO5do8llkIH/uhdjImNkPU743dFyI6Kgli/QMlU2DULM2I6jvpoI5e7zHxLN325C9rp/3eDXD29zTjrdutzS1jT4p9ttLjY/sYeZyWpKs/1GC9633dV/QalFd+qtcULH9Qg9UDV8P/zdd1Y+fFLkyMXo8y7lTNXKPfH2iJfevrWniIN2q2nqdojWfksVA6W59Hm3zyS+GfFsGX3oUr7tZawsjjIJgGI7waSfT+3MOnw6wbtBb60PVam5txJbxzh46UijZtxgeC+Nfx3vsT5I6Aef8SW1ZUBiVTYff7WuM67ho481twwxN6zlrq4KJfak03ceCBSQrf7lksIkHgN8A5QDmwWEQWOudWJ2z6kHPuS36loydOmljMZbNK+f0rm7hsdikTS7oZPdSZtAw4wYtho2brP/jLP9F25SnnwVNeyXn4DM043vm91hLGn67t2+NOhQmL4aUfaklzxLFawo6XkQu1xDq0x56kpbEXf6j/yNESVv4o+Mxz+pg7TANMzXYoHKPt48F0qK+AP16mbcBTL9LM5IOXYfI5XX/GVU9o6TmaAYGWVktna+ZVMBr2DdPMPjMfcobpNi/9SJuzTvkqPPt1rw8hUzODQJo2dUTCWuIe/xF9z5bXNTOJ2viCnq9xp2pAiEQ0GEY7I0uO1kwctB3/6Av0+Z6Net6imU3Zqfp4zve1eaa1QQPshuc1wAwZr/tCNAgMn6bb55RoIBs7T1+ve0abXk74rPaFpGXFxvDvWBoLgjnDNLMdPkNrAB+8rG32oDU4CWp7fVYRzPuCnpvmGihLDASz9Dfzxq/19YhjNHBmFcWagPJHxWphQyfp95FZ4L2eorWX6NXHw6drukfO1OBy3o+1tnTCp+H+q7RD+xN/1ow/e2isxrt/i/5mo7Yt1hrt2d9r/7soLNPCTVM1nPcTKBoXWzfuFB1ZNny6ns9N/4DHP6ff95k3Y5LDt0AAzAU2Ouc2A4jIg8AlQGIg6Be+ef5UXlizm1ueXMmfPnMi0tX1BAcz4SOauW59Q0tSucP1H7KpWv9BRxyjpeF5X4D5P4q9b/YNWipNy4QZl3fc74EawSx9jJZc6yvhxkWaMUaNihv99OlnAdGS2R1nasZ06e9g4Ze1U/eiW7UfYeVjXQeChn3agXrSFzteZzFmbiwQxNcICkq1ZFq9TUvcQ8brusq1GkBA7wWxzruuYt4XYhn61tdh1nWa4S++C179mZ7HGZdrB2vNdi0BL39Aj1U4Tl8H07WZ6ugLtHZwxxmQP1L3mzcy1qkM2qwXPadj5uq+hkzQwHH1H/U8Zw3Rczdypn7u4TN0KO9LP9YBA8d8XJcXlWl7d2aBlqazhmhw+Ncl2vQTDOnnm3JBrMknI08793cu12abx/9Zl0+/XH8/8aI1kw9ehpnXaQCILt/4gn6P0WVR0aAFkJau52D3+xqkcofr8nP/Sy9cjPYvlB6vzWHv/UlHFe37QD9b/mgNWok1gpd/rJ/1hH/W8zZkghYEisog5/hYjTReWkYssJSdCpv+rkE4owA+8p+x5lbTp/wMBKVAfA9aOXBiJ9tdISKnA+uBf3POdeh1E5EFwAKAsWPH+pBUKMnL4D8/NoXvPLmKBxdv4xNzD/E4GXlw9Z+0XTqaaQ6ZqCXF4slaBW+p1xJpvPxRcOHPO+4vfr8SjP0TjZql7bLH3wijj+/6fdF/rOKJ8NkX9HXxRP0Hbq3XUuSMy7WfoH4PXHa7ZujhZu143LZYmyIi4Y7NVeAFpF94//wluizT6ys45srYdq1NOiKncq0+Ijp/UzQQjD9dg9m4k2Ol3Ld+qxfJjT0Zzv3v2KR7+zbHSuUX/UqbP4JpWhv78C0NIH/5gmawe9brXzTT7ky0Wa3IC1bxGfGJn4sF3WAajDlBS7HFk2JBecQxen6OuVJrQIGgBuP0nNiUJaEsGHZ0++Ne5HXWRtr06t/0bC2ZJyo5Ci6/S98/4pjY8mggyB/V9WeLGjFDA8Hw6bFtx5/evoQP2key+E79XipWa2EmmKY1yjd/o9dU5AzVwkHNdjjru7FRbaNm628oWjM5mBmX67kcNk1rPDveizWhmT7lZyDoib8CDzjnmkXkc8B9wEcTN3LO3QHcATBnzhzfJuW/9sRxPLdaawXjh+Ywb0Lxoe8s/h9zyAT9pyoYo9Xko87t/f6Kxuu0ByGvTT4tHa7r5dQBJXFzKk2ZH3s+/ycarF74nma+Z3xTL2TatUKbFNY9rZ9h5HEd9znpHLj2EW3OiHb8RTuN44Uy9TNUrtVmnuKJOkIHtPlh2FR9Xnaq9lm88lN4+X+1FH3N/Xo+o8NUt72jQ0Anna2d41Fj52lm9eZtGiQuvk3bxbe+pk0SXZ6XqTri6tirOq477yftX489WTOvY66KfccX/VK/m+gFXZVr4aRetHYGgjDzE91vc+zHOy6L1hTySw9+jBHHaK1n2PTutxt3qpbO//oVDbzH36jLz/1v7e8JN2lmP/QoDdrx5/+j39ZaTk9r00VlcONTur93fq+1TgsESeFnINgOjIl7PdpbdoBzLr5n7y7gf31Mz0EFA8Jt187m8t++zoI/LOHeT89l9tiig7/xYE79Ny1pBQ6jb/78n2qp0w+hTDjly1rK3vh3HZGya4WOZR93Cjz2WTj+U53/gwcCscB2oEZQ2PlxSo7WcfiRsLaR543QZodxJ8X2Pe0SWHy3XqiWMwwuvjW2Lm+UNp29cau273/sh+3TNO5knX7hhe/qZ5h1vZbMF/4rHDW/Y3riP0NiDa0rR1+gE6rFZ9zRUn+0hgDt28z9Mso7Rk8CQXQIa7Tfoytp6dpEuPJRzeSj/SpTL+rYZJVoyPhYE2Bv5AzVfrFNL+lIN9Pn/AwEi4HJIjIeDQDXANfGbyAiI51zO72XFwNJn6CkICvEvf80l+vvfpvr73qb3143mzOmDDu8nY6YERu5caiCoVjbul8mfVQz4Hf/TzPcmddqJveFTq6A7Uxi01Ci4dO1dhHK0bZuEbjxr+0DR/4o+NJirUGl58b6HUAz7KLx2h4fHQEUb8xcb7uQjuIS0ZLwgpd6lv6eGD5N09eZ7CGxdvLSbprrjpTcEs2suwtyUeNO0cDZWfNeohM+o8NAz/n+4aexpyaeqReY7Vyhv4GcobHJ+g6nAHU4nNMh3unZvXtfa6P2UUkQsgq176i5VgdHpOdoIaa5Tq9k3/SiNg2OO1k/d2aB/kWbdJ3T610+eEVrUGWn+VLI8C0QOOfCIvIl4G9AELjHObdKRH4ALHHOLQS+LCIXA2FgH3CjX+npjTFDsnnk8ydx4z2L+fS9i/nWBdP49Cllh96BPFBMOlsDwcrHdOqMxCm5DyYaCDprGgI4+UtaExh7cuyfK74DN0ok1heSaMgEDQTxwxWjsoq0aaKzUVd9Zcw87WgtHHfwbY+Ei2/t2XbBNO3s74lxJ+v1Mn1p4lnw+q/g996IqcKx0LBfg8C0SzQTjER0/qrMfO1nW/2kfs9zP6cj4SSgHeF1u7UzPyNf+9Y+fFPnyMos0H6W0XN1RN3ejXqRZEae/t7ySzWTbtyvyz54RWvG0y7V2lAgqIMO6ip03c7leiwJ6LpAmmbc+zb1svYuQEKLd0YBZBXouqqtWnhqrYfT/sOXQCBuINwHN86cOXPckiU+3REsQX1zmK89vJxnV+3iqjmj+a9LZ5CRNohHNUQiOlFdwx645Lc6cqc3GvbBvRfAlffE2vyPtKV/0CtUr304eSXF7jTs04wkWYFooHJOmyWba7Q0veM9zawbq/Qallbvwrv0PO+56PxgO947+J3wJBCrNe16v/1V4GWn6bEr12iQCYS0FN9Uo31qY07UfqbEuwOm52qfWcFo7fOKtOm1Gy6ig0KGTdPnjft1xGBGnq5vadB+voxcDUxjT9aRVOVL9HM0VetnbqrSx+ZaHYI+81rvvuWRnnfGJ54GkXedc512wlggOIhIxPHLv2/g1r9vYEZpPr+8ehaThvXyOoOB5PEFeiXwTRsP+QdnzBEViWgJXwLah9HWqq/Tc7SJpXxxrG+irkIDSFqWZpzNNVpLKIjrR6ndHcuc80fGlodbtFSfWMBoqtbpWCJhqNmpxx09x/+m2iPMAsER8NyqXXz9sRXUNYeZP2MkXzhjIlNHdtEEMpBVl+uFWpPOSnZKjDFHUHeBINnDRweMc6ePYOaYQn738iYefbecZ1fu5KtnH8WnTxlPVvogai4qGK1/xpiUYTWCQ7C/voVv/2UlT7+/k6LsEDeePJ7PnDae3AyLq8aY/smahnzgnOOdD/Zx56ubeWFNBUXZIc6YMoyPTR/OOdNGEAwM8hFGxpgBxZqGfCAinDihmBMnFLNsWxV3v/YBr6yv5In3tjOhJIdrThjDxceVMqIgM9lJNcaYblmN4AhqizieWbmTu179gGXbqhCBeeOLOWvqMOZNKGbayHwCVlMwxiSBNQ0lwQd76nly2Xb+unwHmyp1DHRhdohLZ5Zy48lllA3t5cVaxhhzGCwQJNmu6ibe3LyHF9dW8szKnbS2OSaW5HDs6EImDcvl+HFFzBxTSGZoEI0+Msb0KxYI+pFd1U08tWIHr23cw4bddWyv0hvWpwcDzBxTyLnThzN+aA7hiGNGaQGlhVlJTrExZjCwQNCPVTW0sGTLfhZv2cerG/awemdNu/UjCzKZUzaEjxxVwjGlBaQFhdLCLKs9GGN6xQLBALJtXwP7G1qIOFi+rYrFW/bx9gf7qKyN3Xw+IFBWnMPk4bkMz89kTFE2Zx49jKz0IJGIY8yQXs6WaIwZ9CwQDHCRiGP1zhq27m2gpa2ND/Y0sH5XLRsqatlT10J1Y2u77aePyqesOIe2iKMwO0RRTjpDstMpykln0rBcji0tsNFLxqQYu45ggAsEhBmlBcwoLeh0/Y6qRl5aV4kINLS0sej9nazdVUNAhOrGVvY3tNDaFgv4hdkhxhRlM2lYLqdMGsrumiaaWts4cXwxw/MzyAwFyUgLkBEKkp0eJBTsh7N8GmOOGKsRpADnHHXNYfbVt7BsWxVvbd7Lzuomlm+rYn+D1iaCAaEt0vG3IALD8jIoLcxiVGEWI/IzGVGQSXZ6Gqt3VjMkO53LZo8mKxQkLSgMyU6ntilMY2sbBVmhwTUPkzEDmDUNmU61RRwbKmoZWZBFMCAs3bqf6sZWmsMRmlrbaGpto7YpzI6qRnZUN7Kjqomd1Y00tUYAyM1Io6ElTHz8ENHp3aNK8jIoK86mICtEfmaI/CzvLzPtwGNNY5jqxlZmjytiTFEWdc1h6pvbyM9KY+yQbPY3tNISjjA8P2Pw3xzIGJ9Y05DpVDAgHD0iNpX26UeVHPQ9zjlqGsPUNLVSWpjFzpom/rFmN8FAgJZwG3vqWijM1ppAVUMrH+ypp3x/AzuqmljbVEtNYyu1zWF6Wv7ISAvQHNbAk5eRRk5GGsGAEAwIaQHRwIN3R0OBIq8vJNon4nA0tbQxrjiH0UVZ5Gak0dIWobGljeZwhMxQgOz0NLLTg1Q3ttLa5pg8PJfinHTCEceu6iYizpGdnkZORpChuRkdmsqihSkLUmag8jUQiMh84FforSrvcs79OGF9BvAH4HhgL3C1c26Ln2kyh0dEKMgOUZCtN+UoLczihpPKerWPSMRR2xymprGV6sZW8jI1g39r816qGlrJyQiSk57G3voWNuyuY1RhJulpATZV1NHUGiEccUSc08eIQ0TT1RaJsL++lW37GlhRXsW++hZEhIxggNrm3tw6sGsZaQGmjsynJC+D/fUtrNtVS11LmFAwQEluBpmhADkZaQcCSbjNkZ0eJCs9SHM4wuodNTjnGJqXQXFOOrmZIUJBIT0YIC0ohIIB0oMBQtG/NEEQwm0RWtsitLQ52iIRCrPTKcpOJ+L0XIhos5wI1DWFqW0Ok5sRZExRNhGnNbXMUICKmmZa2iKMKswiJz2N5nAbO6qaCAZEa21ZaeRn6vebl5HWLrg552htc7S2RQi3OVojkdjzNv1eAgKF2ekUZoVI66JvqTncRlNLhMz0wOC+498A4lsgEJEg8BvgHKAcWCwiC51zq+M2+wyw3zk3SUSuAX4CXO1Xmkz/EPAynYKsEGPill947KgjehznZZDOOSrrmqmoaaauOUx6WoDMtCAZoQDNrREaWsLUt7SRn6m1jQ2766hpakWAEQVZhIJCfUsb9c1hNlXUsXpnDR/ubSAvM43LZpdSkBWiJRyhsraZ5nCEuuYwlXXNpAUChILCrppWGlvaEIHZ44pIDwbYU9dMRW2zNxJMM9TWNkdrOOJlsK7TPpv0YIBAgAPNc37KSNOg1F16upMVCuJwRJymu2xoNlUNrZTvbzywTUFW6EA/1PHjijh+3BAmDcvVY3vHzwgFyAhqH5QICPoYFb9MiNXMJLrOamoH5WeNYC6w0Tm3GUBEHgQuAeIDwSXA97znjwK3iYi4gdZxYfqlAxmCCMPyMhmW17OZYI8dXehnsnqsLaIlbecgFNTmsOhnamxpo6qxhYAIAS/Y7WtoAbTvJjcjjdqmMNv2NxAKBmiLOBpb2xiWl0F6MMCO6iYaW9oIBYWRBVlEnKOmqfVAs19VQwuVtc2EI+5AbSUtoJlzWkBICwZID+pjWkAO1GDCkQhVDa3sq2+hvjlMIKDpa2wJs3lPPeOKc/j48WPIzUyjoTnMnrpmHPDhvgaeWrGTB97Z1s0ZOTzReBANFgcCBbEVicvig4tE3xy/jy6CD3GBCToGK+LfIz0/5ifmjuWzp004oucF/A0EpUD8t1oOnNjVNs65sIhUA8VAu7tRi8gCYAHA2LFj/UqvMf2K9oV03nSSlR4kK7399CPD8tsHusLs9C4vLpw8PO/IJPIIikQcGyrqKN/fQEs4QktbhObWCM1tEVrCEcJtkQP9QQ4tK0aLjM45bzkHljvilnkbus7Wxe+z3bLYNl0eM/417bcnbvv4Yyamo8MxD6yL9T9Fdzc0N+Owz3NnBkRnsXPuDuAO0FFDSU6OMcYHgYAwZUQeU0b0vyA12Pl5pdB2aNcEPNpb1uk2IpIGFKCdxsYYY/qIn4FgMTBZRMaLSDpwDbAwYZuFwKe851cC/7D+AWOM6Vu+NQ15bf5fAv6GDh+9xzm3SkR+ACxxzi0E7gb+KCIbgX1osDDGGNOHfO0jcM4tAhYlLLsl7nkT8HE/02CMMaZ7NpuYMcakOAsExhiT4iwQGGNMirNAYIwxKW7ATUMtIpXA1kN8+1ASrlruR/pr2vpruqD/pq2/pgv6b9osXb3X27SNc851OsXwgAsEh0NElnQ1H3ey9de09dd0Qf9NW39NF/TftFm6eu9Ips2ahowxJsVZIDDGmBSXaoHgjmQnoBv9NW39NV3Qf9PWX9MF/Tdtlq7eO2JpS6k+AmOMMR2lWo3AGGNMAgsExhiT4lImEIjIfBFZJyIbReQbSUzHGBF5UURWi8gqEfmKt/x7IrJdRJZ5f+cnKX1bROR9Lw1LvGVDROR5EdngPRb1cZqmxJ2XZSJSIyJfTdY5E5F7RKRCRFbGLev0HIm61fvdrRCR2X2crp+KyFrv2E+ISKG3vExEGuPO3e1+paubtHX5/YnIzd45WyciH+vjdD0Ul6YtIrLMW95n56ybfMKf35nejm1w/6HTYG8CJgDpwHJgWpLSMhKY7T3PA9YD09B7N/9HPzhXW4ChCcv+F/iG9/wbwE+S/F3uAsYl65wBpwOzgZUHO0fA+cAz6C1n5wFv93G6zgXSvOc/iUtXWfx2STpnnX5/3v/DciADGO/97wb7Kl0J6/8fcEtfn7Nu8glffmepUiOYC2x0zm12zrUADwKXJCMhzrmdzrml3vNaYA167+b+7BLgPu/5fcClSUzLWcAm59yhXl1+2Jxzr6D3z4jX1Tm6BPiDU28BhSIysq/S5Zx7zjkX9l6+hd4psM91cc66cgnwoHOu2Tn3AbAR/R/u03SJiABXAQ/4cezudJNP+PI7S5VAUApsi3tdTj/IfEWkDJgFvO0t+pJXrbunr5tf4jjgORF5V0QWeMuGO+d2es93AcOTkzRAb14U/4/ZH84ZdH2O+tNv79NoqTFqvIi8JyIvi8hpSUpTZ99ffzlnpwG7nXMb4pb1+TlLyCd8+Z2lSiDod0QkF3gM+Kpzrgb4HTARmAnsRKukyXCqc242cB7wRRE5PX6l03poUsYci97y9GLgEW9Rfzln7STzHHVFRL4FhIH7vUU7gbHOuVnAvwN/FpH8Pk5Wv/z+4nyC9oWOPj9nneQTBxzJ31mqBILtwJi416O9ZUkhIiH0y73fOfc4gHNut3OuzTkXAe7Ep6rwwTjntnuPFcATXjp2R6uZ3mNFMtKGBqelzrndXhr7xTnzdHWOkv7bE5EbgQuB67zMA6/ZZa/3/F20Hf6ovkxXN99ffzhnacDlwEPRZX19zjrLJ/Dpd5YqgWAxMFlExnulymuAhclIiNfueDewxjn387jl8e15lwErE9/bB2nLEZG86HO0o3Eleq4+5W32KeDJvk6bp10JrT+cszhdnaOFwCe9UR3zgOq4qr3vRGQ+8J/Axc65hrjlJSIS9J5PACYDm/sqXd5xu/r+FgLXiEiGiIz30vZOX6YNOBtY65wrjy7oy3PWVT6BX7+zvugB7w9/aK/6ejSKfyuJ6TgVrc6tAJZ5f+cDfwTe95YvBEYmIW0T0NEay4FV0fMEFAN/BzYALwBDkpC2HGAvUBC3LCnnDA1GO4FWtC32M12dI3QUx2+83937wJw+TtdGtO04+lu73dv2Cu87XgYsBS5Kwjnr8vsDvuWds3XAeX2ZLm/5vcDnE7bts3PWTT7hy+/MppgwxpgUlypNQ8YYY7pggcAYY1KcBQJjjElxFgiMMSbFWSAwxpgUZ4HAmD4kImeIyFPJTocx8SwQGGNMirNAYEwnROR6EXnHm3f+9yISFJE6EfmFNz/830WkxNt2poi8JbE5/6NzxE8SkRdEZLmILBWRid7uc0XkUdH7BNzvXUVqTNJYIDAmgYhMBa4GTnHOzQTagOvQq5uXOOemAy8D3/Xe8gfg6865Y9GrOqPL7wd+45w7DjgZvYIVdCbJr6Lzy08ATvH9QxnTjbRkJ8CYfugs4HhgsVdYz0In94oQm4TsT8DjIlIAFDrnXvaW3wc84s3ZVOqcewLAOdcE4O3vHefNYSN696sy4DX/P5YxnbNAYExHAtznnLu53UKR7yRsd6jzszTHPW/D/g9NklnTkDEd/R24UkSGwYH7xI5D/1+u9La5FnjNOVcN7I+7SckNwMtO7ypVLiKXevvIEJHsPv0UxvSQlUSMSeCcWy0i30bv1BZAZ6b8IlAPzPXWVaD9CKDTAd/uZfSbgX/ylt8A/F5EfuDt4+N9+DGM6TGbfdSYHhKROudcbrLTYcyRZk1DxhiT4qxGYIwxKc5qBMYYk+IsEBhjTIqzQGCMMSnOAoExxqQ4CwTGGJPi/j/Lbu0pOPChWQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
